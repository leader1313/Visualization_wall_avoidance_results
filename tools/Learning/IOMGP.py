import numpy as np
import torch
import copy


class IOMGP:
    def __init__(self, X, Y, kernel, Max_mixture=2, K=[], lengthscale=1.0):
        """
        Argument
                X : State (N X Q)
                Y : Action (N X D)
                kernel : kernel function of Gaussian process
                M : Max_mixture : Maximum number of mixtures
                T : Max_learning_iter : Max learning iteration
                K : Number of data generated by each episode
            N : Number of data
            D : Dimension of Action
            prob_thresh : Threshhold of allocate probability z
            Psi : Heteroscedastic gaussian noise variance (N X 1)vector
            update_Psi : New Heteroscedastic gaussian noise variance ((N-N') X 1)vector
                        that we want to update
        """
        self.X = X
        self.Y = Y
        self.N = self.X.shape[0]
        self.M = Max_mixture
        self.D = self.Y.shape[1]
        self.prob_thresh = self.N * (1 / self.M) * 0.1
        lengthscales = torch.log((self.X.max() - self.X.min()) * lengthscale).unsqueeze(
            0
        )
        f_param = lengthscales
        self.kern = []
        for m in range(self.M):
            kernel.__init__(param=f_param)
            self.kern += [copy.deepcopy(kernel)]
        self.kern = np.array(self.kern)  # f: policy function

        self.alpha = torch.tensor(np.log(100)).float()
        self.K = K
        self.update_K()

        # self.log_sigma = torch.stack(
        #     [torch.tensor(np.log([0.5] * self.D)).float() for _ in range(len(self.K))]
        # )  # K X D
        self.log_sigma = torch.stack(
            [torch.tensor(np.log([0.8] * self.D)).float() for _ in range(len(self.K))]
        )  # K X D

        # M X N
        self.q_z_pi = torch.ones(self.M, self.N) / self.M

        self.v_beta_a = torch.ones(self.M)
        self.v_beta_b = torch.ones(self.M) * torch.exp(self.alpha)
        # M X N X D
        self.q_f_mean = torch.tensor(
            np.random.normal(0, 0.01, (self.M, self.N, self.D))
        ).float()
        # D X M X N X N
        self.q_f_sig = torch.stack(
            [
                torch.stack([torch.eye(self.N) for m in range(self.M)])
                for d in range(self.D)
            ]
        )

    def update_K(self):
        if self.K.nelement() == 0:
            self.K = torch.LongTensor([self.N])
        else:
            old_N = self.K.sum()
            new_N = torch.LongTensor([self.N - old_N])
            self.K = torch.cat([self.K, new_N])

    def update_Psi(self):
        # N X D
        Psi = torch.cat(
            [self.log_sigma[k].exp().repeat(self.K[k], 1) for k in range(len(self.K))]
        )
        return Psi

    def precompute(self, n_batch=None):
        """
        Pre-calculations that do not need to be repeated each time to calculate
        """
        if n_batch is None:
            ind = torch.arange(self.N)
        else:
            ind = torch.randperm(self.N)[:n_batch]

        Psi = self.update_Psi()[ind, :]
        Psi = Psi.repeat(self.M, 1, 1)  # M X N' X D

        # Expectation of likelihood
        # p = 1e-45  # scale
        q_z_pi = copy.deepcopy(self.q_z_pi[:, ind])  # M X N'
        q_z_pi = q_z_pi.repeat(self.D, 1, 1).permute(1, 2, 0)  # M X N' X D
        q_z_pi[q_z_pi != 0] /= Psi[q_z_pi != 0]
        # q_z_pi[q_z_pi < p] += p
        B = q_z_pi.permute(2, 0, 1).diag_embed()  # D X M X N'X N'

        sqrtB = torch.sqrt(B)  # D X M X N' X N'

        K = torch.stack(
            [
                self.kern[m].K(self.X[ind]) + torch.eye(ind.shape[0]) * 1e-5
                for m in range(self.M)
            ]
        )  # M X N X N

        R = sqrtB.matmul(K).matmul(sqrtB) + torch.eye(ind.shape[0]).repeat(
            self.D, self.M, 1, 1
        )

        self.RsB = torch.solve(sqrtB, R)[0]
        self.sBRsB = sqrtB.matmul(self.RsB)

        self.sqrtB = sqrtB
        self.R = R
        self.Psi = self.update_Psi()

        return K, B, sqrtB, R

    def update_q_z(self):
        """
        Psi: [N X D] | E_ln_v: [M] | E_ln_1_minus_v: [M]
        q_f_mean: [M X N X D] | q_f_sig: [D X M X N X N]
        q_z_pi: [M X N] | ln_rho: [M X N]
        """
        Psi = self.update_Psi()

        E_ln_v = torch.digamma(self.v_beta_a) - torch.digamma(
            self.v_beta_a + self.v_beta_b
        )
        E_ln_1_minus_v = torch.digamma(self.v_beta_b) - torch.digamma(
            self.v_beta_a + self.v_beta_b
        )

        # term1:-0.5*log 2pi*diag(Sigma)
        term1 = -0.5 * torch.log(np.pi * 2 * Psi).repeat(self.M, 1, 1).sum(2)

        # term2:-0.5*1/Psi*(a-f)^2
        term2 = -0.5 * (
            (((self.Y.repeat(self.M, 1, 1) - self.q_f_mean) ** 2) / Psi).sum(2)
            + torch.diagonal(self.q_f_sig, dim1=2, dim2=3)
            .permute(1, 2, 0)
            .mul(1 / Psi)
            .sum(2)
        )

        # term3: E(lnvPi_{1}^{m-1}(1-v))
        tmp_sum = torch.zeros(self.M)
        for m in range(0, self.M):
            tmp_sum[m] += E_ln_v[m]
            for i in torch.arange(0, m):
                tmp_sum[m] += E_ln_1_minus_v[i]

        term3 = (tmp_sum).repeat(self.N, 1).T

        ln_rho = term1 + term2 + term3
        ln_rho -= ln_rho.max(0)[0]
        self.q_z_pi = torch.exp(ln_rho)
        self.q_z_pi /= self.q_z_pi.sum(0)[None, :]

    def update_q_f(self):
        """
        Psi: [N X D] | K: [M X N X N] | sqrtK: [M X N X N]
        B: [D X M X N X N] | q_f_sig: [D X M X N X N]
        q_z_pi: [M X N] | ln_rho: [M X N]
        """

        K, B, sqrtB, R = self.precompute()

        self.q_f_sig = K - K.matmul(self.sBRsB).matmul(K)

        self.q_f_mean = (
            self.q_f_sig.matmul(B)
            .matmul(self.Y.repeat(self.M, 1, 1).permute(2, 0, 1).unsqueeze(3))
            .squeeze(3)
            .permute(1, 2, 0)
        )

    def update_q_v(self):
        alpha = torch.exp(self.alpha)
        for m in range(0, self.M):
            self.v_beta_a[m] = 1.0 + self.q_z_pi[m, :].sum()
            tmpSum = torch.zeros(1)
            for j in range(m + 1, self.M):
                tmpSum += self.q_z_pi[j, :].sum()

            self.v_beta_b[m] = alpha + tmpSum

    def compute_grad(self, flag):
        self.alpha.requires_grad = flag
        self.log_sigma.requires_grad = flag
        for m in range(self.M):
            if self.q_z_pi[m].sum() > self.prob_thresh:
                self.kern[m].compute_grad(flag)

    def expectation(self, Max_iter=10):
        for _ in range(Max_iter):
            self.update_q_f()
            self.update_q_v()
            self.update_q_z()

            # if torch.isnan(self.q_z_pi).any():
            #     import ipdb
            #     ipdb.set_trace()

    def save_checkpoint(self):
        """
        save checkpoint for load optimized parameters when we need it
        """
        kernels = []
        alpha = [self.alpha]

        log_sigma = self.log_sigma
        q_z_pi = self.q_z_pi
        for m in range(self.M):
            kernels += self.kern[m].param()

        torch.save(
            {
                "q_z_pi": q_z_pi,
                "alpha": alpha,
                "log_sigma": log_sigma,
                "kernels": kernels,
            },
            "checkpoint.pt",
        )

    def load_checkpoint(self):
        """
        load checkpoint for load optimized parameters when we need it
        """
        checkPoint = torch.load("checkpoint.pt")

        self.q_z_pi = checkPoint["q_z_pi"]
        self.alpha = checkPoint["alpha"]
        self.log_sigma = checkPoint["log_sigma"]
        kernels = checkPoint["kernels"]
        for m in range(self.M):
            self.kern[m].LengthScales = kernels[m]

    def variationalBound(self, n_batch=None):

        if n_batch is None:
            ind = torch.arange(self.N)
        else:
            ind = torch.randperm(self.N)[:n_batch]

        alpha = torch.exp(self.alpha)

        Psi = self.update_Psi().repeat(self.M, 1, 1)

        K, B, sqrtB, R = self.precompute(n_batch=n_batch)

        # likelihood term 1-------------------
        Y = self.Y[ind].T.repeat(self.M, 1, 1).permute(1, 0, 2).unsqueeze(2)
        lk_1 = (
            torch.slogdet(R)[1].sum()
            + 0.5 * Y.matmul(self.sBRsB).matmul(Y.transpose(2, 3)).sum()
        )

        # likelihood term 2-------------------

        term1 = (
            torch.log(np.pi * 2 * Psi).permute(2, 0, 1).view(self.D, self.M * self.N)
        )
        term2 = self.q_z_pi.view(self.M * self.N)

        lk_2 = 0.5 * (term1.mul(term2).sum())

        E_zv = torch.zeros(1)
        kl_v = torch.zeros(1)

        v_beta_a = torch.ones(self.M)
        v_beta_b = torch.ones(self.M) * alpha

        for m in range(0, self.M):
            v_beta_a[m] = 1.0 + self.q_z_pi[m][ind].sum()
            tmpSum = torch.zeros(1)
            for i in range(m + 1, self.M):
                tmpSum += self.q_z_pi[i][ind].sum()
            v_beta_b[m] = alpha + tmpSum

        E_ln_v = torch.digamma(v_beta_a) - torch.digamma(v_beta_a + v_beta_b)
        E_ln_1_minus_v = torch.digamma(v_beta_b) - torch.digamma(v_beta_a + v_beta_b)

        # likelihood term 3: E_zv(p(z|v)) -------------------
        tmp_sum = torch.zeros(self.M)
        r = copy.deepcopy(self.q_z_pi)
        r[r != 0] = torch.log(r[r != 0])

        for m in range(0, self.M):
            tmp_sum[m] += E_ln_v[m]
            for i in range(0, m - 1):
                tmp_sum[m] += E_ln_1_minus_v[i]

            E_zv += (self.q_z_pi[m][ind] * (tmp_sum[m])).sum()
            E_zv -= (self.q_z_pi[m][ind][None, ...].mm(r[m][ind][..., None])).sum()

        # likelihood term 4: KL(v*|v) -------------------
        for m in range(0, self.M):
            kl_v -= (
                torch.lgamma(v_beta_a[m])
                + torch.lgamma(v_beta_b[m])
                - torch.lgamma(v_beta_a[m] + v_beta_b[m])
            )
            kl_v += torch.lgamma(alpha) - torch.lgamma(1 + alpha)
            kl_v += (v_beta_a[m] - 1) * (E_ln_v[m]) + (v_beta_b[m] - alpha) * (
                E_ln_1_minus_v[m]
            )

        return lk_1 + lk_2 - E_zv + kl_v

    def maximization(self, max_iter=10):

        self.compute_grad(True)
        param = [self.alpha]
        param += [self.log_sigma]
        # print("param: ", param)
        for m in range(self.M):
            if self.q_z_pi[m].sum() > self.prob_thresh:
                param += self.kern[m].param()
        print("param: ", param)
        # optimizer = torch.optim.Adam(param, lr=0.1)
        # optimizer = torch.optim.Adadelta(param)
        optimizer = torch.optim.Adadelta(param, lr=4.0)
        # optimizer = torch.optim.Adagrad(param, lr=1e-2)
        # optimizer = torch.optim.Adagrad(param, lr=5e-3)
        for i in range(max_iter):
            optimizer.zero_grad()
            try:
                f = self.variationalBound(n_batch=10)
                f.backward()
            except (RuntimeError):
                import ipdb

                ipdb.set_trace()

            optimizer.step()
            with torch.no_grad():
                # Clamping whole parameters
                for parameter in param:
                    min = torch.tensor(np.log(0.00005))
                    max = torch.tensor(1000)
                    parameter.clamp_(min, max)
                # Clamping covariance
                min = torch.tensor(np.log(0.00005))
                max = torch.tensor(np.log(5.0))
                param[1].clamp_(min, max)

        self.compute_grad(False)

    def learning(self, max_iter=10):
        NL = 10000
        self.save_checkpoint()
        step = 0
        stop_flag = False
        Max_patient = 5
        patient_count = 0
        while (step < max_iter) and not (stop_flag):
            step += 1
            print("=========================")
            print("E step")
            self.expectation(Max_iter=10)
            print("M step")
            self.maximization(max_iter=10)

            print(step, " th NL : ", self.variationalBound())
            print("Sigma : ", torch.exp(self.log_sigma[0]))
            print("Alpha : ", torch.exp(self.alpha))
            print("Z : ", self.q_z_pi.sum(axis=1))
            if NL > self.variationalBound():
                patient_count = 0
                NL = self.variationalBound()
                self.save_checkpoint()

            else:
                patient_count += 1
                print("-------Patient_Count(< %i) : %i" % (Max_patient, patient_count))
                if patient_count >= Max_patient:
                    stop_flag = True

        self.load_checkpoint()

        print(self.q_z_pi.sum(axis=1))
        M = self.q_z_pi.sum(axis=1) > self.prob_thresh

        M = M.numpy()

        num_Mixture = M.sum()

        self.kern = self.kern[M]

        self.q_z_pi = self.q_z_pi[self.q_z_pi.sum(axis=1) > self.prob_thresh]
        self.M = num_Mixture
        self.precompute()
        print("-------------------------------------------")
        print("Number of Mixture : %i" % (num_Mixture))
        print("Optimized Noise : ", (np.exp(self.log_sigma[len(self.K) - 1])))

    def predict(self, x):
        """
        Kx: [M X N* X N*] | Knx: [M X N* X N]
        """

        Kx = torch.stack([self.kern[m].K(x) for m in range(self.M)])
        Knx = torch.stack([self.kern[m].K(self.X, x) for m in range(self.M)])

        mean = (
            Knx.transpose(1, 2)
            .matmul(self.sBRsB)
            .matmul(self.Y.repeat(self.M, 1, 1).permute(2, 0, 1).unsqueeze(3))
            .squeeze(3)
            .permute(1, 2, 0)
        )

        sigma = (Kx - Knx.transpose(1, 2).matmul(self.sBRsB).matmul(Knx)).diagonal(
            dim1=2, dim2=3
        ).permute(1, 2, 0) + self.Psi[-1, :].repeat(self.M, x.shape[0], 1)

        sigma[sigma < 0.00001] = 0.00001
        return mean, sigma


if __name__ == "__main__":
    # import sys
    from kernel import GaussianKernel
    import matplotlib.pyplot as plt

    plt.style.use("ggplot")

    N = 100

    # X_limit = [-torch.tensor(np.pi),torch.tensor(np.pi)]
    X_limit = [-torch.tensor(3.0), torch.tensor(3.0)]

    X = torch.linspace(min(X_limit), max(X_limit), N)[:, None]

    Y1 = torch.sin(X) + torch.randn(N)[:, None] * 0.15
    Y2 = torch.cos(X) + torch.randn(N)[:, None] * 0.15

    # K = [100,100]
    # K = [100]
    K = torch.tensor([])
    Max_learning_iter = 10
    kern = GaussianKernel()

    model = IOMGP(
        torch.cat([X, X]).float(),
        torch.cat([Y1, Y2]).float(),
        kern,
        Max_mixture=4,
        K=K,
    )

    model.learning(max_iter=10)

    # GP_ok = model.q_z_pi.sum(axis = 1) > model.prob_thresh
    print(model.q_z_pi.sum(axis=1))
    GP_ok = model.q_z_pi.sum(axis=1) / N > 0.5
    num_graph = GP_ok.sum()
    xx = torch.linspace(min(model.X)[0], max(model.X)[0], steps=100).unsqueeze(1)
    mm, ss = model.predict(xx)
    noise = torch.exp(model.log_sigma[-1])
    noise = torch.sqrt(noise)
    # print(noise)
    # noise is 1-dimensional

    mm = mm[GP_ok]
    ss = ss[GP_ok]
    ss = torch.sqrt(ss)
    plt.cla()

    plt.xlim(min(model.X), max(model.X))
    plt.ylim(-1.5, 1.5)
    xticks = torch.linspace(-3, 3, steps=5)
    yticks = torch.linspace(-1.5, 1.5, steps=5)
    plt.xticks(xticks, fontsize=0)
    plt.yticks(yticks, fontsize=0)

    for m in range(num_graph):
        line = plt.plot(xx, mm[m], color="black", linewidth=1)
        plt.fill_between(
            xx.squeeze(),
            mm[m, :, 0] + ss[m, 0],
            mm[m, :, 0] - ss[m, 0],
            color=line[0].get_color(),
            alpha=0.5,
        )
        # if m == 0:
        #     plt.fill_between(
        #         xx.squeeze(),
        #         mm[m, :, 0] + noise,
        #         mm[m, :, 0] - noise,
        #         color="blue",
        #         alpha=0.2,
        #         hatch="\\",
        #         label="Optimized\nNoise Level",
        #     )
        # else:
        #     plt.fill_between(
        #         xx.squeeze(),
        #         mm[m, :, 0] + noise,
        #         mm[m, :, 0] - noise,
        #         color="blue",
        #         alpha=0.2,
        #         hatch="\\",
        #     )
    plt.scatter(model.X, model.Y, c="tomato", marker="*", s=200, label="Training Data")
    # plt.legend(loc=9, bbox_to_anchor=(0.5, 1.05), fontsize=20, framealpha=0.0, ncol=2)
    # plt.savefig('Figure/Optimization/'+str(Max_learning_iter+1),transparent = False)
    plt.show()
